#!/usr/bin/env python3
"""
Test Page Content
Check what content is actually returned from Time Out Bangkok
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º tools –≤ Python path
sys.path.insert(0, str(Path('.') / 'tools'))

from fetchers.base import get_html


def main():
    """Test the page content."""
    print("üß™ Testing Page Content...")
    print("=" * 50)
    
    url = "https://www.timeout.com/bangkok/things-to-do/best-things-to-do-in-bangkok"
    print(f"üì° Testing: {url}")
    
    try:
        soup = get_html(url)
        if not soup:
            print("‚ùå Failed to get HTML")
            return
        
        print("‚úÖ HTML received successfully")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        title = soup.find('title')
        if title:
            title_text = title.get_text()
            print(f"üìÑ Page title: {title_text}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        all_images = soup.find_all('img')
        print(f"üì∏ Total images found: {len(all_images)}")
        
        if all_images:
            print("  First 5 images:")
            for i, img in enumerate(all_images[:5]):
                src = img.get('src', '')
                alt = img.get('alt', '')
                print(f"    {i+1}. src: {src[:50]}...")
                print(f"       alt: {alt[:30]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        all_headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        print(f"üìù Total headings found: {len(all_headings)}")
        
        if all_headings:
            print("  First 5 headings:")
            for i, heading in enumerate(all_headings[:5]):
                text = heading.get_text(strip=True)
                tag = heading.name
                print(f"    {i+1}. <{tag}> {text[:50]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        all_paragraphs = soup.find_all('p')
        print(f"üìÑ Total paragraphs found: {len(all_paragraphs)}")
        
        if all_paragraphs:
            print("  First 3 paragraphs:")
            for i, p in enumerate(all_paragraphs[:3]):
                text = p.get_text(strip=True)
                if text and len(text) > 10:
                    print(f"    {i+1}. {text[:80]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        all_links = soup.find_all('a', href=True)
        print(f"üîó Total links found: {len(all_links)}")
        
        if all_links:
            print("  First 5 links:")
            for i, link in enumerate(all_links[:5]):
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if text and len(text) > 3:
                    print(f"    {i+1}. {text[:30]}... -> {href[:50]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä HTML
        html_text = soup.get_text()
        print(f"üìä Total text length: {len(html_text)} characters")
        print(f"üìä First 200 characters: {html_text[:200]}...")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
