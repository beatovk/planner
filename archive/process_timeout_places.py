#!/usr/bin/env python3
"""
Process Time Out Bangkok Places
Process collected places through the integrated pipeline
"""

import sys
import json
from pathlib import Path
from typing import Dict, List, Any

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path('.') / 'src'))

from integration import create_places_pipeline
from cache import CacheConfig


def main():
    """Process Time Out Bangkok places through the pipeline."""
    print("üöÄ Processing Time Out Bangkok Places...")
    print("=" * 60)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        print("üîß Initializing integrated pipeline...")
        pipeline = initialize_pipeline()
        if not pipeline:
            print("‚ùå Failed to initialize pipeline")
            return 1
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –º–µ—Å—Ç
        print("üìÇ Loading collected places...")
        places = load_collected_places()
        
        if not places:
            print("‚ùå No places to process")
            return 1
        
        print(f"üìä Loaded {len(places)} places for processing")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Å—Ç —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω
        print("üîó Processing places through pipeline...")
        results = process_places_through_pipeline(pipeline, places)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("üìà Analyzing results...")
        analyze_results(results)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        print("üíæ Saving processing results...")
        save_processing_results(results)
        
        # –û—á–∏—Å—Ç–∫–∞
        print("üßπ Cleaning up...")
        pipeline.close()
        
        print("‚úÖ Time Out Bangkok places processing completed!")
        return 0
        
    except Exception as e:
        print(f"‚ùå Error in processing: {e}")
        import traceback
        traceback.print_exc()
        return 1


def initialize_pipeline():
    """Initialize the integrated places pipeline."""
    try:
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Redis Cloud
        redis_config = CacheConfig(
            host="redis-14374.crce194.ap-seast-1-1.ec2.redns.redis-cloud.com",
            port=14374,
            password="G0vadDS1N9IaEoqQLukwSEGdAHUuPiaW",
            db=0,
            default_ttl=7 * 24 * 3600,
            long_ttl=14 * 24 * 3600,
            key_prefix="v1:places:timeout_bangkok"
        )
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        pipeline = create_places_pipeline(
            db_path="data/timeout_bangkok_places.db",
            redis_config=redis_config,
            min_quality_score=0.7,
            require_photos=True
        )
        
        print("   ‚úì Pipeline initialized successfully")
        return pipeline
        
    except Exception as e:
        print(f"   ‚ùå Pipeline initialization failed: {e}")
        return None


def load_collected_places():
    """Load collected places from the latest file."""
    try:
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª —Å —Å–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –º–µ—Å—Ç–∞–º–∏
        results_dir = Path('results')
        if not results_dir.exists():
            print("   ‚ùå Results directory not found")
            return []
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å pipeline –¥–∞–Ω–Ω—ã–º–∏
        pipeline_files = list(results_dir.glob('timeout_bangkok_simple_pipeline_*.json'))
        if not pipeline_files:
            print("   ‚ùå No pipeline files found")
            return []
        
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
        latest_file = max(pipeline_files, key=lambda x: x.stat().st_mtime)
        print(f"   üìÅ Loading from: {latest_file.name}")
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            places = json.load(f)
        
        print(f"   ‚úì Loaded {len(places)} places from {latest_file.name}")
        return places
        
    except Exception as e:
        print(f"   ‚ùå Error loading places: {e}")
        return []


def process_places_through_pipeline(pipeline, places):
    """Process collected places through the integrated pipeline."""
    try:
        print("   üîó Processing places through pipeline...")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–µ—Å—Ç–∞ —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω
        results = pipeline.process_batch(places)
        
        print(f"   ‚úì Pipeline processing completed: {len(results)} results")
        return results
        
    except Exception as e:
        print(f"   ‚ùå Pipeline processing failed: {e}")
        return []


def analyze_results(results):
    """Analyze pipeline processing results."""
    try:
        print("   üìà Analyzing pipeline results...")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
        status_counts = {}
        for result in results:
            status = result.status
            status_counts[status] = status_counts.get(status, 0) + 1
        
        print("     Pipeline Results:")
        for status, count in status_counts.items():
            print(f"       ‚Ä¢ {status}: {count} places")
        
        # –î–µ—Ç–∞–ª–∏ –ø–æ –º–µ—Å—Ç–∞–º
        print("     Place Details:")
        for result in results:
            if result.status == 'new':
                print(f"       ‚úì {result.name}")
                if hasattr(result, 'quality_metrics') and result.quality_metrics:
                    print(f"         Quality Score: {result.quality_metrics.get_overall_score()}")
                if hasattr(result, 'search_indexed') and result.search_indexed:
                    print(f"         Search Indexed: Yes")
                if hasattr(result, 'cache_updated') and result.cache_updated:
                    print(f"         Cache Updated: Yes")
            elif result.status == 'duplicate':
                print(f"       üîÑ {result.name} (Duplicate)")
                if hasattr(result, 'dedup_info') and result.dedup_info:
                    print(f"         Duplicate of: {result.dedup_info.get('duplicate_id', 'Unknown')}")
            elif result.status == 'rejected':
                print(f"       ‚ùå {result.name} (Rejected)")
                if hasattr(result, 'errors') and result.errors:
                    print(f"         Reasons: {', '.join(result.errors)}")
        
        print("   ‚úì Results analysis completed")
        
    except Exception as e:
        print(f"   ‚ùå Results analysis failed: {e}")


def save_processing_results(results):
    """Save processing results to files."""
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        json_results = []
        for result in results:
            json_result = {
                'place_id': getattr(result, 'place_id', 'Unknown'),
                'name': getattr(result, 'name', 'Unknown'),
                'city': getattr(result, 'city', 'Unknown'),
                'status': getattr(result, 'status', 'Unknown'),
                'dedup_info': getattr(result, 'dedup_info', {}),
                'quality_metrics': getattr(result, 'quality_metrics', {}),
                'search_indexed': getattr(result, 'search_indexed', False),
                'cache_updated': getattr(result, 'cache_updated', False),
                'processing_time': getattr(result, 'processing_time', 0),
                'errors': getattr(result, 'errors', [])
            }
            json_results.append(json_result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        results_file = results_dir / f'timeout_bangkok_processing_{timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"     ‚úì Processing results saved to {results_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
        report_file = results_dir / f'timeout_bangkok_processing_report_{timestamp}.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Time Out Bangkok Processing Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total Places Processed: {len(results)}\n\n")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
            status_counts = {}
            for result in results:
                status = result.status
                status_counts[status] = status_counts.get(status, 0) + 1
            
            f.write("Processing Results:\n")
            for status, count in status_counts.items():
                f.write(f"  {status}: {count} places\n")
            
            f.write("\nDetailed Results:\n")
            for result in results:
                f.write(f"\n{result.name} ({result.status})\n")
                if hasattr(result, 'quality_metrics') and result.quality_metrics:
                    f.write(f"  Quality Score: {result.quality_metrics.get_overall_score()}\n")
                if hasattr(result, 'errors') and result.errors:
                    f.write(f"  Errors: {', '.join(result.errors)}\n")
        
        print(f"     ‚úì Processing report saved to {report_file}")
        
    except Exception as e:
        print(f"     ‚ùå Error saving processing results: {e}")


if __name__ == "__main__":
    from datetime import datetime
    exit_code = main()
    sys.exit(exit_code)
