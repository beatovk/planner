#!/usr/bin/env python3
"""
Ð¢ÐµÑÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð²ÑÐµÑ… ÑÑÑ‹Ð»Ð¾Ðº Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ Bangkok Art City
"""

import sys
from pathlib import Path

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ñ€Ð½ÐµÐ²ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð² Ð¿ÑƒÑ‚ÑŒ
sys.path.insert(0, str(Path(__file__).parent))

from tools.fetchers.base import get_html
from urllib.parse import urljoin

def test_bacc_links():
    print("ðŸ§ª ÐÐ½Ð°Ð»Ð¸Ð· ÑÑÑ‹Ð»Ð¾Ðº Bangkok Art City...")
    
    ROOT = "https://www.bangkokartcity.org"
    LIST_URL = urljoin(ROOT, "/discover/exhibitions")
    
    print(f"ðŸŒ ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼: {LIST_URL}")
    
    soup = get_html(LIST_URL)
    if not soup:
        print("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ")
        return
    
    # Ð˜Ñ‰ÐµÐ¼ Ð²ÑÐµ ÑÑÑ‹Ð»ÐºÐ¸
    all_links = soup.find_all("a", href=True)
    print(f"ðŸ“‹ Ð’ÑÐµÐ³Ð¾ ÑÑÑ‹Ð»Ð¾Ðº: {len(all_links)}")
    
    # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÑÑÑ‹Ð»ÐºÐ¸
    exhibitions_links = []
    other_links = []
    
    for link in all_links:
        href = link.get("href", "")
        text = link.get_text(strip=True)
        
        # Ð˜Ñ‰ÐµÐ¼ ÑÑÑ‹Ð»ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð²ÐµÑÑ‚Ð¸ Ð½Ð° Ð²Ñ‹ÑÑ‚Ð°Ð²ÐºÐ¸
        if any(keyword in href.lower() for keyword in ["exhibition", "show", "art", "gallery"]):
            exhibitions_links.append((href, text))
        elif any(keyword in text.lower() for keyword in ["exhibition", "show", "art", "gallery", "display"]):
            exhibitions_links.append((href, text))
        else:
            other_links.append((href, text))
    
    print(f"\nðŸŽ¨ ÐŸÐ¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Ð²Ñ‹ÑÑ‚Ð°Ð²ÐºÐ¸: {len(exhibitions_links)}")
    for href, text in exhibitions_links[:10]:
        print(f"  {href} - {text}")
    
    print(f"\nðŸ”— Ð”Ñ€ÑƒÐ³Ð¸Ðµ ÑÑÑ‹Ð»ÐºÐ¸ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10): {len(other_links)}")
    for href, text in other_links[:10]:
        print(f"  {href} - {text}")
    
    # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð½Ð°Ð¹Ñ‚Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ Ð¿Ð¾-Ð´Ñ€ÑƒÐ³Ð¾Ð¼Ñƒ
    print(f"\nðŸ” ÐŸÐ¾Ð¸ÑÐº ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð°Ð¼Ð¸:")
    
    # Ð˜Ñ‰ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸
    headers = soup.find_all(["h1", "h2", "h3", "h4"])
    print(f"ðŸ“„ Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸: {len(headers)}")
    for header in headers[:5]:
        text = header.get_text(strip=True)
        if text:
            print(f"  {header.name}: {text}")
    
    # Ð˜Ñ‰ÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ñ‹ Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼
    paragraphs = soup.find_all("p")
    print(f"ðŸ“ ÐŸÐ°Ñ€Ð°Ð³Ñ€Ð°Ñ„Ñ‹: {len(paragraphs)}")
    for p in paragraphs[:3]:
        text = p.get_text(strip=True)
        if text and len(text) > 20:
            print(f"  {text[:100]}...")

if __name__ == "__main__":
    test_bacc_links()
