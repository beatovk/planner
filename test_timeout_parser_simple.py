#!/usr/bin/env python3
"""
Simple test script for Timeout Bangkok parser.
This script works independently without complex imports.
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin
import re

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleTimeoutParser:
    """Simple parser for testing Timeout Bangkok."""
    
    def __init__(self):
        self.base_url = "https://www.timeout.com/bangkok"
        self.session = None
    
    async def __aenter__(self):
        """Async context manager entry."""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.session:
            await self.session.close()
    
    async def test_food_dining(self):
        """Test parsing food & dining category."""
        print("ğŸ½ï¸ Testing food & dining category...")
        
        try:
            # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ food-drink
            url = f"{self.base_url}/food-drink"
            print(f"ğŸ”— Fetching: {url}")
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    print(f"âœ… Successfully fetched {len(html)} characters")
                    
                    # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ HTML
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Ğ˜Ñ‰ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ ÑÑ‚Ğ°Ñ‚ĞµĞ¹
                    headings = soup.find_all(['h1', 'h2', 'h3', 'h4'])
                    print(f"ğŸ“° Found {len(headings)} headings")
                    
                    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 10 Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ²
                    print("\nğŸ“‹ Sample headings:")
                    for i, heading in enumerate(headings[:10], 1):
                        text = heading.get_text(strip=True)
                        if text and len(text) > 5:
                            print(f"{i}. {text}")
                    
                    # Ğ˜Ñ‰ĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸
                    links = soup.find_all('a', href=True)
                    article_links = []
                    
                    for link in links:
                        href = link.get('href')
                        if href and any(keyword in href for keyword in ['/food-drink/', '/restaurants/', '/cafes/']):
                            text = link.get_text(strip=True)
                            if text and len(text) > 5:
                                article_links.append({
                                    'url': href,
                                    'text': text
                                })
                    
                    print(f"\nğŸ”— Found {len(article_links)} article links")
                    
                    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5 ÑÑÑ‹Ğ»Ğ¾Ğº
                    print("\nğŸ“‹ Sample article links:")
                    for i, link in enumerate(article_links[:5], 1):
                        print(f"{i}. {link['text']}")
                        print(f"   URL: {link['url']}")
                    
                    return {
                        'success': True,
                        'headings_count': len(headings),
                        'article_links_count': len(article_links),
                        'sample_headings': [h.get_text(strip=True) for h in headings[:5] if h.get_text(strip=True)],
                        'sample_links': article_links[:5]
                    }
                    
                else:
                    print(f"âŒ Failed to fetch {url}, status: {response.status}")
                    return {'success': False, 'error': f'HTTP {response.status}'}
        
        except Exception as e:
            print(f"âŒ Error: {e}")
            return {'success': False, 'error': str(e)}
    
    async def test_art_exhibits(self):
        """Test parsing art & exhibits category."""
        print("\nğŸ¨ Testing art & exhibits category...")
        
        try:
            url = f"{self.base_url}/art"
            print(f"ğŸ”— Fetching: {url}")
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    print(f"âœ… Successfully fetched {len(html)} characters")
                    
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Ğ˜Ñ‰ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
                    headings = soup.find_all(['h1', 'h2', 'h3', 'h4'])
                    print(f"ğŸ“° Found {len(headings)} headings")
                    
                    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5 Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ²
                    print("\nğŸ“‹ Sample headings:")
                    for i, heading in enumerate(headings[:5], 1):
                        text = heading.get_text(strip=True)
                        if text and len(text) > 5:
                            print(f"{i}. {text}")
                    
                    return {'success': True, 'headings_count': len(headings)}
                    
                else:
                    print(f"âŒ Failed to fetch {url}, status: {response.status}")
                    return {'success': False, 'error': f'HTTP {response.status}'}
        
        except Exception as e:
            print(f"âŒ Error: {e}")
            return {'success': False, 'error': str(e)}

async def main():
    """Main test function."""
    print("ğŸš€ Starting Timeout Bangkok parser test...")
    print("=" * 50)
    
    async with SimpleTimeoutParser() as parser:
        # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ food & dining
        food_result = await parser.test_food_dining()
        
        # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ art & exhibits
        art_result = await parser.test_art_exhibits()
        
        print("\n" + "=" * 50)
        print("ğŸ“Š Test Results Summary:")
        print(f"ğŸ½ï¸  Food & Dining: {'âœ… Success' if food_result['success'] else 'âŒ Failed'}")
        if food_result['success']:
            print(f"   ğŸ“° Headings: {food_result['headings_count']}")
            print(f"   ğŸ”— Article links: {food_result['article_links_count']}")
        
        print(f"ğŸ¨ Art & Exhibits: {'âœ… Success' if art_result['success'] else 'âŒ Failed'}")
        if art_result['success']:
            print(f"   ğŸ“° Headings: {art_result['headings_count']}")
        
        print("\nğŸ¯ Next steps:")
        print("1. Install aiohttp: pip install aiohttp")
        print("2. Run the full parser: python core/fetchers/timeout_bangkok_parser.py")
        print("3. Use the CLI: python scripts/timeout_places_cli.py test")

if __name__ == "__main__":
    asyncio.run(main())
