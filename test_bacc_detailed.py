#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è Bangkok Art City —Ñ–µ—Ç—á–µ—Ä–∞
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))

from tools.fetchers.base import get_html
from urllib.parse import urljoin

def test_bacc_detailed():
    print("üß™ –î–µ—Ç–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç Bangkok Art City...")
    
    ROOT = "https://www.bangkokartcity.org"
    LIST_URL = urljoin(ROOT, "/discover/exhibitions")
    
    print(f"üåê –ü—Ä–æ–≤–µ—Ä—è–µ–º: {LIST_URL}")
    
    # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—ã—Å—Ç–∞–≤–æ–∫
    soup = get_html(LIST_URL)
    if not soup:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—ã—Å—Ç–∞–≤–æ–∫")
        return
    
    print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤—ã—Å—Ç–∞–≤–æ–∫ –ø–æ–ª—É—á–µ–Ω–∞")
    
    # –®–∞–≥ 2: –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤—ã—Å—Ç–∞–≤–∫–∏
    links = soup.select("a[href*='/exhibitions/'], a[href*='/discover/']")
    print(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
    
    if not links:
        print("‚ùå –°—Å—ã–ª–∫–∏ –Ω–∞ –≤—ã—Å—Ç–∞–≤–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏
        all_links = soup.find_all("a", href=True)
        print(f"üìã –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(all_links)}")
        for i, link in enumerate(all_links[:5]):
            href = link.get("href", "")
            text = link.get_text(strip=True)[:50]
            print(f"  {i+1}. {href} - {text}")
        return
    
    # –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ {min(3, len(links))} —Å—Å—ã–ª–∫–∏:")
    
    for i, link in enumerate(links[:3]):
        href = link.get("href", "")
        text = link.get_text(strip=True)
        print(f"\n  {i+1}. {text}")
        print(f"     –°—Å—ã–ª–∫–∞: {href}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π URL
        url = href if href.startswith("http") else urljoin(ROOT, href)
        print(f"     –ü–æ–ª–Ω—ã–π URL: {url}")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        detail = get_html(url)
        if not detail:
            print("     ‚ùå –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            continue
        
        print("     ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª—É—á–µ–Ω–∞")
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title_el = detail.select_one("h1")
        if title_el:
            title = title_el.get_text(strip=True)
            print(f"     üìÑ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
        else:
            print("     ‚ùå –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ò—â–µ–º –¥–∞—Ç—ã
        time_el = detail.select_one("time[datetime]") or detail.find("time")
        if time_el:
            datetime_attr = time_el.get("datetime")
            time_text = time_el.get_text(strip=True)
            print(f"     üìÖ Time —ç–ª–µ–º–µ–Ω—Ç: datetime='{datetime_attr}', text='{time_text}'")
        else:
            print("     ‚ùå Time —ç–ª–µ–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
            date_texts = []
            for el in detail.find_all(string=True):
                text = str(el).strip()
                if any(year in text for year in ["2025", "2026", "2024"]):
                    date_texts.append(text)
            
            if date_texts:
                print(f"     üìÖ –ù–∞–π–¥–µ–Ω—ã –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ: {date_texts[:3]}")
            else:
                print("     ‚ùå –î–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

if __name__ == "__main__":
    test_bacc_detailed()
