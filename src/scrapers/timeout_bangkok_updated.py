#!/usr/bin/env python3
"""
Updated Time Out Bangkok Scraper
Uses the correct selectors based on current site structure
"""

import sys
import time
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ Python path
sys.path.insert(0, str(Path('.') / 'src'))

from integration import create_places_pipeline
from cache import CacheConfig


def get_timeout_html(url: str, timeout: int = 15):
    """
    –ü–æ–ª—É—á–∞–µ—Ç HTML —Å Time Out Bangkok
    """
    import requests
    from bs4 import BeautifulSoup
    import random
    
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ]
    
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    try:
        session = requests.Session()
        session.headers.update(headers)
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(random.uniform(1, 3))
        
        response = session.get(url, timeout=timeout)
        if response.status_code != 200:
            print(f"HTTP {response.status_code} for {url}")
            return None
        
        return BeautifulSoup(response.text, "html.parser")
        
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None


def extract_places_from_page(url: str, category: str = "general") -> List[Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Time Out Bangkok
    """
    places = []
    
    try:
        print(f"     üìÑ Fetching: {url}")
        soup = get_timeout_html(url)
        if not soup:
            print(f"       ‚ùå Failed to get HTML")
            return places
        
        # –ò—â–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
        articles = soup.find_all('article', class_='_article_a9wsr_1')
        print(f"       üìä Found {len(articles)} articles")
        
        for article in articles:
            try:
                place = extract_place_from_article(article, category)
                if place:
                    places.append(place)
                    
            except Exception as e:
                print(f"       ‚ö†Ô∏è Error extracting place from article: {e}")
                continue
        
        print(f"       ‚úÖ Extracted {len(places)} places")
        return places
        
    except Exception as e:
        print(f"       ‚ùå Error processing page: {e}")
        return places


def extract_place_from_article(article, category: str) -> Optional[Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Å—Ç–µ –∏–∑ —Å—Ç–∞—Ç—å–∏
    """
    try:
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –∏—â–µ–º h3 —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–ª–∞—Å—Å–æ–º
        title_el = article.find('h3', class_='_h3_c6c0h_1')
        if not title_el:
            # –§–æ–ª–±—ç–∫: –ª—é–±–æ–π h3
            title_el = article.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        
        if not title_el:
            return None
        
        title = title_el.get_text(strip=True)
        if not title or len(title) < 5:
            return None
        
        # –°—Å—ã–ª–∫–∞ - –∏—â–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–ª–∞—Å—Å–æ–º
        link_el = article.find('a', class_='_titleLinkContainer_a9wsr_48')
        if not link_el:
            # –§–æ–ª–±—ç–∫: –ª—é–±–∞—è —Å—Å—ã–ª–∫–∞
            link_el = article.find('a')
        
        if not link_el:
            return None
        
        href = link_el.get('href', '')
        if not href:
            return None
        
        # –ü–æ–ª–Ω—ã–π URL
        if href.startswith('/'):
            url = f"https://www.timeout.com{href}"
        elif href.startswith('http'):
            url = href
        else:
            return None
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - –∏—â–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
        image_url = None
        img_el = article.find('img', class_='_image_a9wsr_26')
        if not img_el:
            # –§–æ–ª–±—ç–∫: –ª—é–±–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img_el = article.find('img')
        
        if img_el:
            src = img_el.get('src', '')
            if src and src.startswith('http'):
                image_url = src
        
        # –û–ø–∏—Å–∞–Ω–∏–µ (–∏–∑ alt —Ç–µ–∫—Å—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
        description = None
        if img_el:
            alt = img_el.get('alt', '')
            if alt and len(alt) > 10:
                description = alt
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–ª–∞–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ URL
        flags = determine_flags_from_url(url, category)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Å—Ç–æ
        place = {
            'title': title,
            'url': url,
            'image': image_url,
            'description': description,
            'category': category,
            'flags': flags,
            'source': 'timeout.com',
            'city': 'Bangkok'
        }
        
        return place
        
    except Exception as e:
        print(f"         ‚ö†Ô∏è Error extracting place: {e}")
        return None


def determine_flags_from_url(url: str, category: str) -> List[str]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–ª–∞–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    """
    flags = []
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º URL –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –º–µ—Å—Ç–∞
    url_lower = url.lower()
    
    if any(word in url_lower for word in ['restaurant', 'food', 'dining']):
        flags.extend(['food_dining', 'restaurants'])
    elif any(word in url_lower for word in ['bar', 'pub', 'nightlife', 'club']):
        flags.extend(['nightlife', 'bars'])
    elif any(word in url_lower for word in ['market', 'shopping', 'mall']):
        flags.extend(['shopping', 'markets'])
    elif any(word in url_lower for word in ['museum', 'gallery', 'art', 'exhibition']):
        flags.extend(['culture', 'art', 'museums'])
    elif any(word in url_lower for word in ['park', 'garden', 'nature']):
        flags.extend(['nature', 'parks', 'outdoors'])
    elif any(word in url_lower for word in ['spa', 'wellness', 'yoga', 'fitness']):
        flags.extend(['wellness', 'health'])
    elif any(word in url_lower for word in ['hotel', 'accommodation', 'resort']):
        flags.extend(['accommodation', 'hotels'])
    elif any(word in url_lower for word in ['theater', 'cinema', 'concert', 'show']):
        flags.extend(['entertainment', 'culture'])
    else:
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–ª–∞–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if category in ['food', 'restaurants']:
            flags.extend(['food_dining', 'restaurants'])
        elif category in ['bars', 'nightlife']:
            flags.extend(['nightlife', 'bars'])
        elif category in ['attractions', 'things-to-do']:
            flags.extend(['attractions', 'things_to_do'])
        elif category in ['shopping', 'markets']:
            flags.extend(['shopping', 'markets'])
        else:
            flags.append('attractions')
    
    return flags


def collect_timeout_bangkok_places() -> List[Dict[str, Any]]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Å—Ç–∞ —Å–æ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü Time Out Bangkok
    """
    print("üöÄ Starting Time Out Bangkok Places Collection...")
    print("=" * 60)
    
    all_places = []
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Å–±–æ—Ä–∞
    collection_urls = [
        {
            'url': 'https://www.timeout.com/bangkok',
            'category': 'general'
        },
        {
            'url': 'https://www.timeout.com/bangkok/things-to-do',
            'category': 'attractions'
        }
    ]
    
    # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ URL
    additional_urls = [
        'https://www.timeout.com/bangkok/food-drink',
        'https://www.timeout.com/bangkok/music-nightlife',
        'https://www.timeout.com/bangkok/shopping',
        'https://www.timeout.com/bangkok/events'
    ]
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    for collection in collection_urls:
        print(f"üì° Collecting from: {collection['url']}")
        places = extract_places_from_page(collection['url'], collection['category'])
        all_places.extend(places)
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
        time.sleep(2)
    
    # –ü—Ä–æ–±—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ URL
    for url in additional_urls:
        print(f"üì° Trying additional URL: {url}")
        try:
            places = extract_places_from_page(url, 'general')
            if places:
                all_places.extend(places)
                print(f"       ‚úÖ Got {len(places)} places from {url}")
            else:
                print(f"       ‚ö†Ô∏è No places from {url}")
        except Exception as e:
            print(f"       ‚ùå Error with {url}: {e}")
        
        time.sleep(2)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
    unique_places = []
    seen_urls = set()
    
    for place in all_places:
        if place['url'] not in seen_urls:
            unique_places.append(place)
            seen_urls.add(place['url'])
    
    print(f"üìä Collection Summary:")
    print(f"   Total places found: {len(all_places)}")
    print(f"   Unique places: {len(unique_places)}")
    print(f"   Duplicates removed: {len(all_places) - len(unique_places)}")
    
    return unique_places


def convert_to_pipeline_format(places: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –º–µ—Å—Ç–∞ Time Out –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
    """
    pipeline_places = []
    
    for i, place in enumerate(places):
        try:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            place_id = f"timeout_{i+1}_{int(time.time())}"
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–æ—Ç–æ
            photos = []
            image_url = place.get('image')
            if image_url:
                photos.append({
                    'url': image_url,
                    'width': 1200,
                    'height': 800
                })
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞
            pipeline_place = {
                'id': place_id,
                'name': place.get('title', 'Unknown Place'),
                'city': 'Bangkok',
                'domain': 'timeout.com',
                'url': place.get('url', ''),
                'description': place.get('description', ''),
                'address': 'Bangkok, Thailand',  # Time Out –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—á–Ω—ã–π –∞–¥—Ä–µ—Å
                'geo_lat': None,
                'geo_lng': None,
                'tags': [],
                'flags': place.get('flags', ['attractions']),
                'phone': None,
                'email': None,
                'website': None,
                'hours': None,
                'price_level': None,
                'rating': None,
                'photos': photos,
                'image_url': image_url,
                'quality_score': 0.85,  # –ë–∞–∑–æ–≤—ã–π score –¥–ª—è Time Out
                'last_updated': datetime.now().strftime('%Y-%m-%d')
            }
            
            pipeline_places.append(pipeline_place)
            
        except Exception as e:
            print(f"     ‚ö†Ô∏è Error converting place {i}: {e}")
            continue
    
    print(f"     ‚úì Converted {len(pipeline_places)} places to pipeline format")
    return pipeline_places


def main():
    """Main function to collect and process Time Out Bangkok places."""
    try:
        # –°–±–æ—Ä –º–µ—Å—Ç
        places = collect_timeout_bangkok_places()
        
        if not places:
            print("‚ùå No places collected")
            return 1
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞
        print("\nüîÑ Converting places to pipeline format...")
        pipeline_places = convert_to_pipeline_format(places)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüíæ Saving collected places...")
        save_collected_places(places, pipeline_places)
        
        print("\n‚úÖ Time Out Bangkok collection completed!")
        print(f"üìä Total places collected: {len(places)}")
        print(f"üîÑ Places ready for pipeline: {len(pipeline_places)}")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Error in collection: {e}")
        import traceback
        traceback.print_exc()
        return 1


def save_collected_places(places: List[Dict], pipeline_places: List[Dict]):
    """Save collected places to files."""
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        raw_file = results_dir / f'timeout_bangkok_raw_{timestamp}.json'
        with open(raw_file, 'w', encoding='utf-8') as f:
            json.dump(places, f, indent=2, ensure_ascii=False)
        print(f"     ‚úì Raw data saved to {raw_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞
        pipeline_file = results_dir / f'timeout_bangkok_pipeline_{timestamp}.json'
        with open(pipeline_file, 'w', encoding='utf-8') as f:
            json.dump(pipeline_places, f, indent=2, ensure_ascii=False)
        print(f"     ‚úì Pipeline data saved to {pipeline_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        report_file = results_dir / f'timeout_bangkok_report_{timestamp}.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Time Out Bangkok Collection Report\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Collection Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total Places: {len(places)}\n")
            f.write(f"Pipeline Ready: {len(pipeline_places)}\n\n")
            
            f.write("Collected Places:\n")
            for i, place in enumerate(places, 1):
                f.write(f"\n{i}. {place.get('title', 'Unknown')}\n")
                f.write(f"   URL: {place.get('url', 'No URL')}\n")
                f.write(f"   Category: {place.get('category', 'Unknown')}\n")
                f.write(f"   Flags: {', '.join(place.get('flags', []))}\n")
                if place.get('image'):
                    f.write(f"   Image: Yes\n")
                if place.get('description'):
                    f.write(f"   Description: {place.get('description', '')[:100]}...\n")
        
        print(f"     ‚úì Report saved to {report_file}")
        
    except Exception as e:
        print(f"     ‚ùå Error saving results: {e}")


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
