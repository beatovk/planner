from __future__ import annotations
from typing import Dict, List, Optional
import time
import re
import random
from .base import get_html, normalize_event, parse_date_range

ROOT = "https://www.zipeventapp.com"

def get_zipevent_html(url: str, timeout: int = 20) -> Optional[BeautifulSoup]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç HTML —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è Zipevent
    """
    from bs4 import BeautifulSoup
    import requests
    
    # –†–æ—Ç–∞—Ü–∏—è User-Agent
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    ]
    
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "th-TH,th;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    # Retry –ª–æ–≥–∏–∫–∞
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies
            session = requests.Session()
            session.headers.update(headers)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
            time.sleep(random.uniform(2, 5))
            
            r = session.get(url, timeout=timeout)
            if r.status_code != 200:
                print(f"HTTP {r.status_code} for {url}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # exponential backoff
                    continue
                return None
            return BeautifulSoup(r.text, "lxml")
            
        except requests.exceptions.Timeout:
            print(f"Timeout attempt {attempt + 1}/{max_retries} for {url}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            return None
        except Exception as e:
            print(f"Error fetching {url} (attempt {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            return None
    
    return None

def get_event_urls_from_sitemap() -> List[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL —Å–æ–±—ã—Ç–∏–π –∏–∑ sitemap
    """
    print("üìã –ü–æ–ª—É—á–∞–µ–º sitemap Zipevent...")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º sitemap
        sitemap_url = f"{ROOT}/sitemap.xml"
        soup = get_zipevent_html(sitemap_url)
        
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å sitemap")
            return []
        
        # –ò—â–µ–º –≤—Å–µ URL —Å–æ–±—ã—Ç–∏–π (–ø–∞—Ç—Ç–µ—Ä–Ω /e/)
        event_urls = []
        for loc in soup.find_all("loc"):
            url = loc.get_text(strip=True)
            if "/e/" in url:
                event_urls.append(url)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(event_urls)} —Å–æ–±—ã—Ç–∏–π –≤ sitemap")
        return event_urls
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ sitemap: {e}")
        return []

def parse_event_page(url: str) -> Dict | None:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–æ–±—ã—Ç–∏—è
    """
    try:
        print(f"    üîç –ü–∞—Ä—Å–∏–º: {url}")
        soup = get_zipevent_html(url)
        if not soup:
            print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML")
            return None
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ title –∏–ª–∏ itemprop="name"
        title = None
        
        # –ò—â–µ–º –≤ itemprop="name" (—ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è Zipevent)
        og_title = soup.select_one("meta[itemprop='name']")
        if og_title:
            title = og_title.get("content", "").strip()
            print(f"    üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ itemprop: {title}")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç, –±–µ—Ä–µ–º –∏–∑ title
        if not title:
            title_el = soup.select_one("title")
            if title_el:
                title = title_el.get_text(strip=True).replace(" | Zipevent - Inspiration Everywhere", "").strip()
                print(f"    üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ title: {title}")
        
        if not title:
            print(f"    ‚ùå –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        # –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ itemprop="description" (—ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è Zipevent)
        desc = None
        og_desc = soup.select_one("meta[itemprop='description']")
        if og_desc:
            desc = og_desc.get("content", "").strip()
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ og:image
        image = None
        og_image = soup.select_one("meta[property='og:image']")
        if og_image:
            image = og_image.get("content", "").strip()
        
        # URL
        event_url = url
        
        # –î–∞—Ç–∞ - –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        date_str = None
        end_date = None
        
        # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
        text_content = soup.get_text()
        date_patterns = [
            r'\b\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4}\b',
            r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4}\b',
            r'\b\d{1,2}/\d{1,2}/\d{4}\b',
            r'\b\d{4}-\d{2}-\d{2}\b'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text_content, re.IGNORECASE)
            if matches:
                date_str = matches[0]
                break
        
        if date_str:
            print(f"    üìÖ –ù–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞: {date_str}")
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω
            start_date, end_date = parse_date_range(date_str)
            if start_date:
                date_str = start_date
                print(f"    üìÖ –ü–∞—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∞—Ç–∞: {date_str}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞—Ç–∞ –≤ –±—É–¥—É—â–µ–º (–Ω–µ —Å—Ç–∞—Ä—à–µ 2 –Ω–µ–¥–µ–ª—å)
                from datetime import datetime, timedelta
                try:
                    event_date = datetime.strptime(start_date, "%Y-%m-%d")
                    today = datetime.now()
                    two_weeks_later = today + timedelta(weeks=2)
                    
                    print(f"    üìÖ –î–∞—Ç–∞ —Å–æ–±—ã—Ç–∏—è: {event_date}")
                    print(f"    üìÖ –°–µ–≥–æ–¥–Ω—è: {today}")
                    print(f"    üìÖ 2 –Ω–µ–¥–µ–ª–∏ –≤–ø–µ—Ä–µ–¥: {two_weeks_later}")
                    
                    # –ï—Å–ª–∏ —Å–æ–±—ã—Ç–∏–µ –≤ –ø—Ä–æ—à–ª–æ–º –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ –≤ –±—É–¥—É—â–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    if event_date < today or event_date > two_weeks_later:
                        print(f"    ‚ùå –°–æ–±—ã—Ç–∏–µ –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 2 –Ω–µ–¥–µ–ª—å")
                        return None
                        
                except ValueError:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É")
                    return None
        else:
            print(f"    ‚ùå –î–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        # –ú–µ—Å—Ç–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è - –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
        venue = None
        venue_keywords = ["Bangkok", "Central", "Mall", "Plaza", "Center", "Expo", "Fair", "Market"]
        for keyword in venue_keywords:
            if keyword.lower() in text_content.lower():
                # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å —ç—Ç–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
                sentences = re.split(r'[.!?]', text_content)
                for sentence in sentences:
                    if keyword.lower() in sentence.lower():
                        venue = sentence.strip()[:100]
                        break
                if venue:
                    break
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ/–æ–ø–∏—Å–∞–Ω–∏–∏
        category_hint = "events"
        if any(word in title.lower() for word in ["food", "restaurant", "dining"]):
            category_hint = "food"
        elif any(word in title.lower() for word in ["market", "fair", "expo", "festival"]):
            category_hint = "markets_fairs"
        elif any(word in title.lower() for word in ["workshop", "training", "class"]):
            category_hint = "workshops"
        elif any(word in title.lower() for word in ["yoga", "wellness", "health"]):
            category_hint = "yoga_wellness"
        elif any(word in title.lower() for word in ["music", "concert", "party"]):
            category_hint = "live_music_gigs"
        
        # –¢–µ–≥–∏
        tags = []
        if any(word in title.lower() for word in ["food", "restaurant", "dining"]):
            tags.append("Food")
        if any(word in title.lower() for word in ["market", "fair", "expo"]):
            tags.append("Markets")
        if any(word in title.lower() for word in ["workshop", "training"]):
            tags.append("Workshops")
        if any(word in title.lower() for word in ["yoga", "wellness"]):
            tags.append("Wellness")
        if any(word in title.lower() for word in ["music", "concert"]):
            tags.append("Music")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if not date_str:
            return None
        
        return {
            "title": title,
            "date_iso": date_str,
            "end_date_iso": end_date,
            "url": event_url,
            "source": "zipeventapp.com",
            "category_hint": category_hint,
            "desc": desc,
            "image": image,
            "venue": venue,
            "tags": tags
        }
        
    except Exception as e:
        print(f"Error parsing event {url}: {e}")
        return None

def fetch(cat_id: str = None) -> List[Dict]:
    """
    –§–µ—Ç—á–µ—Ä –¥–ª—è Zipevent –Ω–∞ –æ—Å–Ω–æ–≤–µ sitemap
    –°–∫–∞—á–∏–≤–∞–µ–º sitemap ‚Üí –æ—á–µ—Ä–µ–¥—å URL ‚Üí –ø–∞—Ä—Å–∏–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å–æ–±—ã—Ç–∏–π
    """
    print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Zipevent —Ñ–µ—Ç—á–µ—Ä (sitemap –ø–æ–¥—Ö–æ–¥)...")
    start_time = time.time()
    
    out = []
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ URL —Å–æ–±—ã—Ç–∏–π –∏–∑ sitemap
    event_urls = get_event_urls_from_sitemap()
    
    if not event_urls:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL —Å–æ–±—ã—Ç–∏–π")
        return []
    
    print(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(event_urls)} —Å–æ–±—ã—Ç–∏–π...")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    max_events = 20  # —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø–æ–∏—Å–∫–∞ –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π
    event_urls = event_urls[:max_events]
    
    for i, url in enumerate(event_urls):
        if len(out) >= 5:  # –ª–∏–º–∏—Ç –Ω–∞ —Ñ–µ—Ç—á–µ—Ä - –±—ã–ª–æ 40, —Å–æ–∫—Ä–∞—â–∞–µ–º –¥–æ 5
            break
        
        print(f"  üìÖ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–±—ã—Ç–∏–µ {i+1}/{len(event_urls)}: {url.split('/')[-1]}")
        
        try:
            event_data = parse_event_page(url)
            if event_data:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞
                if cat_id and cat_id != event_data.get("category_hint"):
                    continue
                
                out.append(normalize_event(**event_data))
                print(f"    ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ: {event_data['title']}")
            else:
                print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å")
        
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            continue
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if i < len(event_urls) - 1:
            delay = random.uniform(2, 4)
            time.sleep(delay)
    
    elapsed = time.time() - start_time
    print(f"üéâ Zipevent —Ñ–µ—Ç—á–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω: {len(out)} —Å–æ–±—ã—Ç–∏–π –∑–∞ {elapsed:.1f}—Å")
    
    return out[:5]  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 5 —Å–æ–±—ã—Ç–∏–π - –±—ã–ª–æ 40
